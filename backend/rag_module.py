# rag_module.py
import numpy as np, pandas as pd, faiss, re, os, json
import kagglehub
from config import client  # å¯¼å…¥ client
from utils import safe_text, safe_batch, chunk_text # å¯¼å…¥å·¥å…·
from state import indexes # å¯¼å…¥å…¨å±€ state


def embed_texts(texts, input_type="passage", batch_size=32):
    # TODO: batch_sizeåæœŸå¯ä»¥è°ƒæ•´åˆ°64-128æå‡åå
    all_vecs = []
    clean_texts = safe_batch(texts)
    for i in range(0, len(clean_texts), batch_size):
        batch = clean_texts[i:i+batch_size]
        try:
            resp = client.embeddings.create(
                input=batch,
                model="nvidia/llama-3.2-nv-embedqa-1b-v2",
                encoding_format="float",
                extra_body={"input_type": input_type, "truncate": "NONE"}
            )
            all_vecs.extend([d.embedding for d in resp.data])
        except Exception as e:
            print(f"åµŒå…¥æ‰¹æ¬¡ {i//batch_size+1} å¤±è´¥ï¼š{e}")
    return np.array(all_vecs, dtype="float32")

# ==============================================================
# 4. æ„å»º FAISS ç´¢å¼•ï¼ˆRAG æ£€ç´¢åŸºç¡€ + æŒä¹…åŒ–ï¼‰
# --------------------------------------------------------------
# åŠŸèƒ½ï¼š
#   - è‡ªåŠ¨åˆ†æ®µ + åµŒå…¥ + åŠ å…¥å‘é‡ç´¢å¼•
#   - å¦‚æœå·²æœ‰æŒä¹…åŒ–æ–‡ä»¶ï¼Œåˆ™ç›´æ¥åŠ è½½ï¼Œè·³è¿‡é‡æ–°åµŒå…¥
# ==============================================================

import json

def build_faiss_index(texts, name="empathy", max_chars=1000, overlap=100):
    """
    name: ç´¢å¼•åç§°ï¼ˆç”¨äºä¿å­˜æ–‡ä»¶ï¼‰
    ä¼šç”Ÿæˆï¼š
        ./faiss_{name}.index
        ./faiss_{name}_corpus.json
    """
    index_path = f"./faiss_{name}.index"
    corpus_path = f"./faiss_{name}_corpus.json"

    # ---------- â‘  è‹¥å·²å­˜åœ¨æŒä¹…åŒ–æ–‡ä»¶ï¼Œåˆ™ç›´æ¥åŠ è½½ ----------
    if os.path.exists(index_path) and os.path.exists(corpus_path):
        try:
            index = faiss.read_index(index_path)
            with open(corpus_path, "r", encoding="utf-8") as f:
                corpus = json.load(f)
            print(f"âœ… [{name}] ç´¢å¼•åŠ è½½å®Œæˆï¼š{len(corpus)} æ®µæ–‡æœ¬ï¼Œç»´åº¦ {index.d}ï¼ˆå·²æŒä¹…åŒ–ï¼‰")
            return index, corpus
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½å·²æœ‰ç´¢å¼•ï¼Œé‡æ–°æ„å»ºï¼š{e}")

    # ---------- â‘¡ æ­£å¸¸æ„å»ºæµç¨‹ ----------
    expanded = []
    for t in texts:
        expanded.extend(chunk_text(t, max_chars=max_chars, overlap=overlap))
    corpus = [safe_text(t) for t in expanded if t and t.strip()]
    if not corpus:
        print(f"âš ï¸ [{name}] æ— æœ‰æ•ˆæ–‡æœ¬ï¼Œè·³è¿‡ç´¢å¼•æ„å»ºã€‚")
        return None, []
    print(f"ğŸ”§ [{name}] æ­£åœ¨ä¸º {len(corpus)} æ®µæ–‡æœ¬ç”ŸæˆåµŒå…¥...")
    vecs = embed_texts(corpus, input_type="passage")
    if vecs.size == 0:
        print(f"âš ï¸ [{name}] æœªè·å–åˆ°å‘é‡ï¼Œè·³è¿‡ç´¢å¼•æ„å»ºã€‚")
        return None, []
    dim = vecs.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(vecs)
    print(f"âœ… [{name}] ç´¢å¼•å®Œæˆï¼š{len(corpus)} å‘é‡ï¼Œç»´åº¦ {dim}ï¼ˆå·²æŒä¹…åŒ–ï¼‰")

    # ---------- â‘¢ ä¿å­˜åˆ°å½“å‰ç›®å½• ----------
    faiss.write_index(index, index_path)
    with open(corpus_path, "w", encoding="utf-8") as f:
        json.dump(corpus, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ [{name}] ç´¢å¼•ä¸è¯­æ–™å·²ä¿å­˜è‡³å½“å‰ç›®å½•ã€‚")

    return index, corpus


def extract_user_texts(df):
    users = []
    for row in df["empathetic_dialogues"].astype(str):
        m = re.search(r"Customer\s*:\s*(.*?)(?:Agent|$)", row, re.S)
        if m:
            users.append(m.group(1).strip())
    return users

def extract_agent_texts(df):
    agents = []
    for row in df["empathetic_dialogues"].astype(str):
        m = re.search(r"Agent\s*:\s*(.*)", row)
        if m:
            txt = m.group(1).strip()
            if len(txt) > 3:
                agents.append(txt)
    if "labels" in df.columns:
        agents.extend(df["labels"].dropna().astype(str).tolist())
    return agents

def load_datasets_dual():
    # -------- Empathetic Dialogues --------
    path_e = kagglehub.dataset_download("atharvjairath/empathetic-dialogues-facebook-ai")
    df_e = pd.read_csv(f"{path_e}/emotion-emotion_69k.csv")
    df_e = df_e.loc[:, ~df_e.columns.str.contains('^Unnamed')]

    empathy_users = extract_user_texts(df_e)
    empathy_agents = extract_agent_texts(df_e)

    idx_emp_u, corp_emp_u = build_faiss_index(empathy_users[:1500], "empathy_user")
    idx_emp_a, corp_emp_a = build_faiss_index(empathy_agents[:1500], "empathy_agent")

    # -------- CounselChat --------
    path_c = kagglehub.dataset_download("weiting016/counselchat-data")
    df_c = pd.read_csv(f"{path_c}/counselchat-data.csv")
    users = df_c["questionText"].fillna("").astype(str).tolist()
    agents = df_c["answerText"].fillna("").astype(str).tolist()
    idx_con_u, corp_con_u = build_faiss_index(users[:1500], "counsel_user")
    idx_con_a, corp_con_a = build_faiss_index(agents[:1500], "counsel_agent")

    return {
        "empathy_user": (idx_emp_u, corp_emp_u),
        "empathy_agent": (idx_emp_a, corp_emp_a),
        "counsel_user": (idx_con_u, corp_con_u),
        "counsel_agent": (idx_con_a, corp_con_a),
    }
    
def load_and_init_indexes():
    """
    æ‰§è¡Œä¸€æ¬¡åŠ è½½ï¼Œå¹¶æŠŠç»“æœå­˜å…¥å…¨å±€ state.indexes
    è¿™ä¸ªå‡½æ•°ä¼šè¢« main.py è°ƒç”¨
    """
    try:
        loaded_data = load_datasets_dual()
        indexes.update(loaded_data) # å…³é”®: .update() ä¼šä¿®æ”¹ä» state å¯¼å…¥çš„å­—å…¸
        print("âœ… RAG ç´¢å¼•å·²åŠ è½½åˆ°å…¨å±€çŠ¶æ€ã€‚")
    except Exception as e:
        print(f"âŒ RAG ç´¢å¼•åŠ è½½å¤±è´¥: {e}")

# ==============================================================
# 7. è¯­ä¹‰æ£€ç´¢ï¼ˆRAGï¼‰
# --------------------------------------------------------------
# åŠŸèƒ½ï¼šåŸºäºç”¨æˆ·queryåšå‘é‡æ£€ç´¢ï¼Œæ‹¼æ¥ä¸Šä¸‹æ–‡ç»™LLMå‚è€ƒ
# ==============================================================
def retrieve_context(index, corpus, query, k=3):
    try:
        if not index or not corpus or not query or not str(query).strip():
            return ""
        clean_query = safe_text(query)[:4000]
        qv = embed_texts([clean_query], input_type="query")
        if qv.size == 0:
            return ""
        D, I = index.search(qv, k)
        ctx = [corpus[i] for i in I[0] if 0 <= i < len(corpus)]
        return "\n\n".join(ctx)
    except Exception as e:
        print(f"æ£€ç´¢é”™è¯¯ï¼š{e}")
        return ""

# --- æ–°å¢ä¸€ä¸ªåˆå§‹åŒ–å‡½æ•° ---
def load_and_init_indexes():
    """
    æ‰§è¡Œä¸€æ¬¡åŠ è½½ï¼Œå¹¶æŠŠç»“æœå­˜å…¥å…¨å±€ state.indexes
    è¿™ä¸ªå‡½æ•°åº”è¯¥åœ¨ main.py æˆ– app.py å¯åŠ¨æ—¶è¢«è°ƒç”¨ä¸€æ¬¡
    """
    loaded_data = load_datasets_dual()
    indexes.update(loaded_data)
    print("âœ… RAG ç´¢å¼•å·²åŠ è½½åˆ°å…¨å±€çŠ¶æ€ã€‚")