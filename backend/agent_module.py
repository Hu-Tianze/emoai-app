# agent_module.py
import numpy as np
import re
from config import client # å¯¼å…¥ client
from utils import safe_text
from state import agent_scores, agent_feedback_memory, user_prefs # å¯¼å…¥ state

# 8. å­¦ä¹ å‹äººæ ¼ç³»ç»Ÿ
AGENTS = {
    "empathetic": {
        "role": "empathetic companion",
        "objective": "help the user feel heard and gently supported; keep it conversational",
        "tone": "warm, natural, brief",
        "avoid": "clinical counseling language or over-apologizing unless distress is explicit",
    },
    "counselor": {
        "role": "caring psychological counselor",
        "objective": "validate explicit distress and offer 2-3 small, concrete coping steps",
        "tone": "calm, clear, human",
        "avoid": "repetitive breathing/self-care scripts unless requested; robotic empathy templates",
    },
    "funny": {
        "role": "playful friend",
        "objective": "lighten the mood with short, harmless humor (one-liners)",
        "tone": "light, kind",
        "avoid": "sarcasm, teasing the user personally, dark jokes",
    }
}

# é»˜è®¤å…¨å±€åå¥½ï¼ˆå¯ä»¥ç”± interpret_feedback() åŠ¨æ€ä¿®æ”¹ï¼‰
user_prefs = {
    "tone": "neutral and gentle",
    "reply_length": "medium",
    "positivity": "balanced",
    "empathy_level": "high"
}

def build_global_persona(user_prefs):
    """
    æ„å»ºå…¨å±€äººæ ¼è¯´æ˜ï¼Œç”¨äºæ¯ä¸€è½®Promptçš„å‰ç½®éƒ¨åˆ†ã€‚
    æ‰€æœ‰Agentéƒ½å¿…é¡»éµå®ˆè¿™é‡Œçš„åŸºè°ƒå’Œé£æ ¼ã€‚
    """
    tone = user_prefs.get("tone", "neutral and gentle")
    length = user_prefs.get("reply_length", "medium")
    positivity = user_prefs.get("positivity", "balanced")
    empathy_level = user_prefs.get("empathy_level", "high")

    return f"""
[Global User Persona Settings]
The user prefers replies that are:
- Tone: {tone}
- Length: {length}
- Positivity: {positivity}
- Empathy level: {empathy_level}

Always adapt your replies to maintain this personality baseline,
even when switching between different agents.
"""

def build_prompt(agent_name, context, user_input, global_tone_hint="neutral", prefs=None):
    global user_prefs
    prefs = prefs or user_prefs
    """
    æ„é€ æœ€ç»ˆå‘é€ç»™LLMçš„Promptï¼ŒåŒ…å«ï¼š
      - Agentèº«ä»½ä¸ç›®æ ‡è¯´æ˜
      - è¡Œä¸ºçº¦æŸï¼ˆguardrailsï¼‰
      - ç”¨æˆ·åå¥½ (æ¥è‡ªMetaåé¦ˆæˆ–è¶‹åŠ¿)
      - å…¨å±€è¯­æ°”æç¤º (global_tone_hint)
    """

    spec = AGENTS[agent_name]
    # è‹¥æœªæ˜¾å¼ä¼ å…¥prefsï¼Œåˆ™é»˜è®¤ä½¿ç”¨å…¨å±€ user_prefs
    prefs = prefs or user_prefs or {}

    # è¯»å–ç”¨æˆ·åå¥½å‚æ•°ï¼ˆå¦‚toneã€å›å¤é•¿åº¦ã€æƒ…ç»ªæ­£å‘æ€§ï¼‰
    tone_adjust = prefs.get("tone", "unchanged")
    length_pref = prefs.get("reply_length", "unchanged")
    positivity = prefs.get("positivity", "unchanged")

    # æ‹¼æ¥ç”¨æˆ·åå¥½è¯´æ˜ï¼ˆä»…åœ¨å­˜åœ¨æœ‰æ•ˆåå¥½æ—¶åŠ å…¥ï¼‰
    pref_hint = ""
    if any(v != "unchanged" for v in [tone_adjust, length_pref, positivity]):
        pref_hint = f"""
[User Preference Override]
The user prefers the following adjustments:
- Tone â†’ {tone_adjust}
- Reply length â†’ {length_pref}
- Positivity level â†’ {positivity}
Please adapt your language style accordingly, while keeping consistency with your current role and goal.
"""

    # åŸºç¡€è¡Œä¸ºçº¦æŸ guardrails
    guardrails = """
[Behavioral Guardrails]
- Mirror the user's vibe. If cheerful/playful, NEVER apologize or show pity.
- If neutral, stay warm and concise.
- If mildly negative but not asking for help, be supportive without being clinical.
"""
    if agent_name == "counselor":
        guardrails += """
- Validate pain ONLY if distress/help is explicit.
- Do NOT invent or assume specific details about the user's situation.
- Offer up to 3 concrete, small next steps max.
"""
    if agent_name == "funny":
        guardrails += """
- Keep jokes short and gentle; never target the user; avoid sensitive topics.
"""

    # æœ€ç»ˆPromptç»“æ„
    # æ³¨å…¥åæ€åé¦ˆ
    reflection = ""
    if agent_feedback_memory[agent_name]:
        mem = list(agent_feedback_memory[agent_name])
        reflection = "\nRecent feedback about your replies:\n" + "\n".join([f"- {m}" for m in mem[-3:]])

    return f"""
You are a {spec['role']}.
Your goal is to {spec['objective']}.
Maintain a {spec['tone']} tone (global tone hint: {global_tone_hint}).
Avoid {spec['avoid']}.

{reflection}

{pref_hint}

{guardrails}

[Retrieved Context]
{context}

Always comply with [User Preference Override], [Behavioral Guardrails], and your recent feedback.
"""

def choose_agent_gen3(emo, intent, val, score, prev_val):
    """
    åŠŸèƒ½ï¼šæ ¹æ®å½“å‰æƒ…ç»ªä¸æ„å›¾åŠ¨æ€é€‰æ‹©Agentã€‚
    é€»è¾‘ï¼š
        - æ˜ç¡®æ±‚åŠ© or å¼ºè´Ÿé¢æƒ…ç»ª â†’ counselor
        - æ˜ç¡®è¦æ±‚å¨±ä¹ â†’ funny
        - å¦åˆ™æ ¹æ®softmax(æƒé‡+æƒ…ç»ªåç½®)æŠ½æ ·é€‰æ‹©
    """
    negative = {"sadness","fear","anger","disgust","grief","remorse","disappointment","nervousness"}
    if intent == "help" or (emo in negative and val < -0.6 and score > 0.7):
        return "counselor"
    if intent == "fun":
        return "funny"

    names = list(agent_scores.keys())
    logits = np.array([agent_scores[n] for n in names], dtype="float32")

    tilt = np.array([
        0.10 if names[0]=="empathetic" and val>=0 else 0.0,
        0.10 if names[1]=="counselor" and val<0 else 0.0,
        0.05 if names[2]=="funny" and val>=0.2 else 0.0
    ], dtype="float32")
    logits = logits + tilt
    probs = np.exp(logits) / np.sum(np.exp(logits))
    return np.random.choice(names, p=probs)

def update_agent_reward(agent, prev_val, new_val, user_input, last_reply):
    """
    - åŸºäºæƒ…ç»ªå˜åŒ–ç”Ÿæˆè¯­è¨€åé¦ˆï¼›
    - å­˜å…¥ agent_feedback_memoryï¼›
    - å¥–åŠ±æˆ–æƒ©ç½šé€šè¿‡ prompt-level reflection å®ç°ã€‚
    """
    delta = float(new_val - (prev_val or 0))
    if abs(delta) < 0.05:
        return  # æƒ…ç»ªæ²¡æ˜æ˜¾å˜åŒ–å°±è·³è¿‡

    if delta > 0:
        # å¥–åŠ±ï¼šç”Ÿæˆæ­£å‘åé¦ˆ
        feedback_prompt = f"""
The user's emotional state improved after your last message.
User said: "{user_input}"
Your reply was: "{last_reply}"
Describe in one concise sentence what you did well, so you can repeat it next time.
"""
    else:
        # æƒ©ç½šï¼šç”Ÿæˆåæ€åé¦ˆ
        feedback_prompt = f"""
The user's emotional state got worse or stayed negative after your last message.
User said: "{user_input}"
Your reply was: "{last_reply}"
Describe in one concise sentence what to avoid next time to prevent emotional decline.
"""

    try:
        fb = client.chat.completions.create(
            model="meta/llama-3.1-8b-instruct",
            messages=[{"role": "user", "content": safe_text(feedback_prompt)}],
            temperature=0.3,
            max_tokens=80
        )
        feedback = fb.choices[0].message.content.strip()
        agent_feedback_memory[agent].append(feedback)
        print(f"ğŸ§  {agent.capitalize()} feedback â†’ {feedback}")
    except Exception as e:
        print(f"[update_agent_reward feedback gen failed] {e}")

# --- å¤„ç† Meta åé¦ˆçš„å‡½æ•° ---
def update_agent_meta_feedback(agent: str, user_input: str, meta_key: str, meta_meaning: str):
    """
    å½“æ£€æµ‹åˆ° Meta åé¦ˆæ—¶ï¼Œç”Ÿæˆé’ˆå¯¹è¡Œä¸ºè°ƒæ•´çš„åæ€ï¼Œå¹¶å­˜å…¥ feedback memoryã€‚
    è¿™ä¸ªå‡½æ•° *ä¸* è°ƒæ•´ agent_scoresã€‚
    """
    # æ„æ€ä¸€ä¸ªè®© LLM ç”Ÿæˆè¡Œä¸ºè°ƒæ•´å»ºè®®çš„ Prompt
    meta_feedback_prompt = f"""
The user provided feedback about your behavior: "{user_input}"
This feedback implies: "{meta_meaning}" (Keyword: {meta_key})

Based on this, describe in one concise sentence a specific action you should take or avoid in the future to better meet the user's preference. Start your sentence with "I should..." or "I will try to...".
Example: If user said "too long", you might say "I should provide more concise answers."
Example: If user said "too formal", you might say "I will try to use a friendlier tone."
"""
    try:
        # è°ƒç”¨ LLM ç”Ÿæˆåæ€
        fb = client.chat.completions.create(
            model="meta/llama-3.1-8b-instruct",
            messages=[{"role": "user", "content": safe_text(meta_feedback_prompt)}],
            temperature=0.3,
            max_tokens=60
        ).choices[0].message.content.strip()

        # æ¸…ç†å¹¶æ ¼å¼åŒ–åé¦ˆ
        # ç¡®ä¿ä»¥ "I should/will" å¼€å¤´ï¼Œå»é™¤å¤šä½™å¼•å·
        fb_cleaned = re.sub(r'^["\']|["\']$', '', fb) # å»é™¤é¦–å°¾å¼•å·
        if not fb_cleaned.lower().startswith(("i should", "i will")):
             feedback_text = "ğŸ“ [Meta Feedback] I should " + fb_cleaned.lower() # å¼ºåˆ¶åŠ ä¸Šå‰ç¼€
        else:
             feedback_text = "ğŸ“ [Meta Feedback] " + fb_cleaned[0].upper() + fb_cleaned[1:] # é¦–å­—æ¯å¤§å†™

        # ç¡®ä¿åªæœ‰ä¸€ä¸ªå¥å­
        feedback_text = feedback_text.split('.')[0] + '.'

        # å­˜å…¥å¯¹åº” Agent çš„ feedback memory
        agent_feedback_memory[agent].append(feedback_text)
        print(f"ğŸ“ Meta åæ€å·²ç”Ÿæˆå¹¶è®°å½•: {feedback_text}")

    except Exception as e:
        print(f"ğŸ“ Meta åæ€ç”Ÿæˆå¤±è´¥: {e}")
        
def update_agent_meta_feedback(agent: str, user_input: str, meta_key: str, meta_meaning: str):
    """
    å½“æ£€æµ‹åˆ° Meta åé¦ˆæ—¶ï¼Œç”Ÿæˆé’ˆå¯¹è¡Œä¸ºè°ƒæ•´çš„åæ€ï¼Œå¹¶å­˜å…¥ feedback memoryã€‚
    
    """
    # æ„æ€ä¸€ä¸ªè®© LLM ç”Ÿæˆè¡Œä¸ºè°ƒæ•´å»ºè®®çš„ Prompt
    meta_feedback_prompt = f"""
The user provided feedback about your behavior: "{user_input}"
This feedback implies: "{meta_meaning}" (Keyword: {meta_key})

Based on this, describe in one concise sentence a specific action you should take or avoid in the future to better meet the user's preference. Start your sentence with "I should..." or "I will try to...".
Example: If user said "too long", you might say "I should provide more concise answers."
Example: If user said "too formal", you might say "I will try to use a friendlier tone."
"""
    try:
        # è°ƒç”¨ LLM ç”Ÿæˆåæ€
        fb = client.chat.completions.create(
            model="meta/llama-3.1-8b-instruct",
            messages=[{"role": "user", "content": safe_text(meta_feedback_prompt)}],
            temperature=0.3,
            max_tokens=60
        ).choices[0].message.content.strip()

        # æ¸…ç†å¹¶æ ¼å¼åŒ–åé¦ˆ
        fb_cleaned = re.sub(r'^["\']|["\']$', '', fb) # å»é™¤é¦–å°¾å¼•å·
        if not fb_cleaned.lower().startswith(("i should", "i will")):
             # å¦‚æœ LLM æ²¡æŒ‰è¦æ±‚ç”Ÿæˆï¼Œå¼ºåˆ¶åŠ ä¸Šå‰ç¼€
             feedback_text = "ğŸ“ [Meta Feedback] I should " + fb_cleaned[0].lower() + fb_cleaned[1:]
        else:
             feedback_text = "ğŸ“ [Meta Feedback] " + fb_cleaned[0].upper() + fb_cleaned[1:] # ç¡®ä¿é¦–å­—æ¯å¤§å†™

        # ç¡®ä¿åªæœ‰ä¸€ä¸ªå¥å­ï¼Œå»é™¤å¯èƒ½çš„åç»­å†…å®¹
        feedback_text = feedback_text.split('.')[0] + '.'

        # å­˜å…¥å¯¹åº” Agent çš„ feedback memory
        agent_feedback_memory[agent].append(feedback_text)
        print(f"ğŸ“ Meta åæ€å·²ç”Ÿæˆå¹¶è®°å½•: {feedback_text}")

    except Exception as e:
        print(f"ğŸ“ Meta åæ€ç”Ÿæˆå¤±è´¥: {e}")
# --- ç»“æŸæ–°å¢ ---